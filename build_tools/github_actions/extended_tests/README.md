# TheRock Extended Tests Framework

Unified testing framework for TheRock ROCm distribution, supporting benchmark and functional testing with automated execution, system detection, and results management.

## Table of Contents

- [Overview](#overview)
- [Quick Start](#quick-start)
- [Project Structure](#project-structure)
- [CI/CD Integration](#cicd-integration)
- [Architecture](#architecture)

## Overview

The test framework provides infrastructure for two test types:

| Test Type                     | Purpose                          | Result Types         | When to Use                                  | Status        |
| ----------------------------- | -------------------------------- | -------------------- | -------------------------------------------- | ------------- |
| **[Benchmark](benchmark/)**   | Performance regression detection | PASS/FAIL/UNKNOWN    | Prevent performance degradation (nightly CI) | âœ… Implemented |
| **[Functional](functional/)** | Correctness validation           | PASS/FAIL/ERROR/SKIP | Verify expected behavior (nightly CI)        | ğŸš§ Under Development |

### Key Features

- **Shared Infrastructure** - Common utilities, configuration, and results management
- **System Auto-Detection** - Hardware, OS, GPU, and ROCm version detection
- **Results Management** - Local storage (JSON) and API upload with retry logic
- **Comprehensive Logging** - File rotation and configurable log levels
- **Error Handling** - Custom exceptions with clear, actionable messages
- **Modular Architecture** - Extensible design for adding new test types
- **CI/CD Integration** - Parallel execution in nightly CI

## Quick Start

### Environment Setup

All tests require these environment variables. **Note:** These are automatically configured in CI runs. For local testing, adjust values based on your setup:

```bash
# Required: Update to your actual TheRock build directory
export THEROCK_BIN_DIR=/path/to/therock/build/bin

# Optional: Unique identifier for this test run (default: local-test)
export ARTIFACT_RUN_ID=local-test

# Required: Update to match your GPU family (e.g., gfx908, gfx90a, gfx942, gfx950-dcgpu)
export AMDGPU_FAMILIES=gfx950-dcgpu

# Optional: Control GPU visibility on multi-GPU nodes (e.g., ROCR_VISIBLE_DEVICES=0)
# export ROCR_VISIBLE_DEVICES=0
```

### Running Tests

See test-specific READMEs for detailed instructions and examples:

- **[Benchmark Tests](benchmark/README.md)** - Performance regression testing
- **[Functional Tests](functional/README.md)** - Correctness validation testing (under development)

## Project Structure

```
extended_tests/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ README.md                       # This file
â”‚
â”œâ”€â”€ configs/                        # SHARED configuration
â”‚   â””â”€â”€ config.yml                 # Framework config (logging, API, execution)
â”‚
â”œâ”€â”€ benchmark/                      # Benchmark tests (LKG comparison)
â”‚   â”œâ”€â”€ scripts/                   # Test implementations
â”‚   â”‚   â”œâ”€â”€ benchmark_base.py      # Base class with LKG logic
â”‚   â”‚   â””â”€â”€ test_*_benchmark.py    # Individual benchmark tests
â”‚   â”œâ”€â”€ configs/                   # Test-specific configurations
â”‚   â”‚   â”œâ”€â”€ hipblaslt.json
â”‚   â”‚   â””â”€â”€ rocfft.json
â”‚   â”œâ”€â”€ benchmark_test_matrix.py   # Benchmark test matrix
â”‚   â””â”€â”€ README.md                  # Benchmark-specific docs
â”‚
â”œâ”€â”€ functional/                     # Functional/correctness tests (ğŸš§ under development)
â”‚   â””â”€â”€ README.md                  # Functional-specific docs (placeholder - tests to be added in follow-up PRs)
â”‚
â””â”€â”€ utils/                          # SHARED utilities for all test types
    â”œâ”€â”€ exceptions.py              # Custom exception classes
    â”‚   â”œâ”€â”€ BenchmarkExecutionError   # Execution/parsing failures
    â”‚   â”œâ”€â”€ BenchmarkResultError      # Result validation failures
    â”‚   â””â”€â”€ FrameworkException        # Base exception
    â”‚
    â”œâ”€â”€ logger.py                  # Logging utilities
    â”œâ”€â”€ test_client.py             # Test execution client
    â”œâ”€â”€ constants.py               # Global constants
    â”‚
    â”œâ”€â”€ config/                    # Configuration parsers
    â”‚   â”œâ”€â”€ config_parser.py
    â”‚   â”œâ”€â”€ config_validator.py
    â”‚   â””â”€â”€ config_helper.py
    â”‚
    â”œâ”€â”€ results/                   # Results handling & LKG
    â”‚   â”œâ”€â”€ results_api.py        # API for storing/retrieving results
    â”‚   â””â”€â”€ results_handler.py    # Process and format results
    â”‚
    â””â”€â”€ system/                    # Hardware & ROCm detection
        â”œâ”€â”€ hardware.py           # GPU detection and capabilities
        â”œâ”€â”€ platform.py           # Platform-specific utilities
        â””â”€â”€ rocm_detector.py      # ROCm version detection
```

## CI/CD Integration

### Test Execution Schedule

| Workflow Trigger           | Benchmark Tests | Functional Tests |
| -------------------------- | --------------- | ---------------- |
| **Pull Request (PR)**      | Skipped         | Skipped          |
| **Nightly CI (scheduled)** | Run (parallel)  | ğŸš§ Under Development |
| **Push to main**           | Skipped         | Skipped          |

### Parallel Execution Architecture

Tests run in **parallel** for faster CI execution:

```
ci_nightly.yml
  â””â”€ ci_linux.yml / ci_windows.yml
      â”œâ”€ build_artifacts
      â”‚
      â”œâ”€ test_artifacts â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚   â””â”€ Component tests (smoke/full)   â”‚ Run in parallel
      â”‚                                      â”‚ after build
      â”œâ”€ test_benchmarks â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
      â”‚   â””â”€ Benchmark tests                â”‚
      â””â”€ test_functional_tests â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â””â”€ Functional tests (ğŸš§ under development)
```

**Workflow Files:**

- `.github/workflows/ci_nightly.yml` - Nightly CI orchestration
- `.github/workflows/ci_linux.yml` / `ci_windows.yml` - Platform-specific CI logic
- `.github/workflows/test_benchmarks.yml` - Benchmark test execution (uses `benchmark_runs_on`)
- `.github/workflows/test_functional_tests.yml` - Functional test execution (ğŸš§ under development, uses `test_runs_on`)
- `.github/workflows/test_artifacts.yml` - Component test execution (uses `test_runs_on`)

**Key Differences:**

- **Component Tests**: Run on all PRs (smoke) and nightly (full), use regular runners
- **Benchmark Tests**: Run only on nightly, use dedicated performance runners (`benchmark_runs_on`)
- **Functional Tests**: ğŸš§ Under development - will run only on nightly, use regular runners (`test_runs_on`)

## Architecture

### Common Test Execution Flow

All tests follow this pattern:

1. **Initialize** - Auto-detect system (GPU, ROCm), load configuration, setup logging
1. **Execute** - Run test binaries/scripts, capture output to log files
1. **Parse** - Extract metrics/results from logs, structure data
1. **Process** - Type-specific validation (LKG comparison or correctness check)
1. **Report** - Display results, upload to API, update GitHub Actions summary

### Implementation Details

See test-specific READMEs for detailed implementation guides:

- **[Benchmark Tests](benchmark/README.md)** - LKG comparison logic and adding new benchmarks
- **[Functional Tests](functional/README.md)** - Correctness validation and adding new tests (ğŸš§ under development)
- **[Shared Utils](utils/README.md)** - Common utilities, exceptions, and helpers
