#!/usr/bin/env python3

"""
Upload Python packages to S3 or a local directory.

Usage:
  upload_python_packages.py
    --packages-dir PACKAGES_DIR
    --artifact-group ARTIFACT_GROUP
    --run-id RUN_ID
    [--output-dir OUTPUT_DIR]  # Local output instead of S3
    [--bucket BUCKET]          # Override bucket selection
    [--dry-run]                # Print what would happen

This script uploads built Python packages (wheels, sdists) to S3 for testing
by downstream workflows. It can also output to a local directory for testing.

Modes:
  1. S3 upload (default): Uploads to S3 bucket selected by retrieve_bucket_info()
  2. Local output: With --output-dir, copies files to local directory
  3. Dry run: With --dry-run, prints plan without uploading or copying

S3 Layout:
  {bucket}/{external_repo}{run_id}-{platform}/python/{artifact_group}/
    *.whl, *.tar.gz   # Wheel and sdist files
    simple/           # PEP-503 pip index (generated by piprepo)

"""

import argparse
from pathlib import Path
import platform
import shlex
import shutil
import subprocess
import sys

from github_actions_utils import (
    gha_append_step_summary,
    retrieve_bucket_info,
)

THEROCK_DIR = Path(__file__).resolve().parent.parent.parent
PLATFORM = platform.system().lower()


def log(*args):
    print(*args)
    sys.stdout.flush()


def run_command(cmd: list[str], cwd: Path = Path.cwd()):
    log(f"++ Exec [{cwd}]$ {shlex.join(cmd)}")
    subprocess.run(cmd, check=True)


def run_piprepo_build(dist_dir: Path, dry_run: bool = False):
    """Generates a PEP-503 pip index in dist_dir/simple/ using piprepo."""
    if not shutil.which("piprepo"):
        raise FileNotFoundError(
            "piprepo not found. Install it with: pip install piprepo"
        )

    cmd = ["piprepo", "build", str(dist_dir)]

    if dry_run:
        log(f"[DRY RUN] Would run: {shlex.join(cmd)}")
        return

    run_command(cmd)


# TODO: share helper with post_build_upload.py? (that accepts files or dirs)
# TODO: switch to boto3? (just matching existing upload behavior for now)
def run_aws_cp(source_path: Path, s3_destination: str, dry_run: bool = False):
    """Uploads a directory to S3."""
    if not source_path.is_dir():
        raise ValueError(f"source_path must be a directory: {source_path}")

    cmd = ["aws", "s3", "cp", str(source_path), s3_destination, "--recursive"]

    if dry_run:
        log(f"[DRY RUN] Would run: {shlex.join(cmd)}")
        return

    run_command(cmd)


# TODO: share helper with post_build_upload.py?
def run_local_cp(source_path: Path, dest_path: Path, dry_run: bool = False):
    """Copies a directory to a local destination."""
    if not source_path.is_dir():
        raise ValueError(f"source_path must be a directory: {source_path}")

    if dry_run:
        log(f"[DRY RUN] Would copy {source_path} -> {dest_path}")
        return

    log(f"[INFO] Copying {source_path} -> {dest_path}")
    dest_path.parent.mkdir(parents=True, exist_ok=True)

    if dest_path.exists():
        shutil.rmtree(dest_path)
    shutil.copytree(source_path, dest_path)


def find_package_files(dist_dir: Path) -> list[Path]:
    """Finds all wheel, sdist, and index files in the dist directory."""
    files = []
    for pattern in ["*.whl", "*.tar.gz", "**/*.html"]:
        files.extend(dist_dir.glob(pattern))

    return sorted(files)


def upload_packages(
    run_id: str,
    artifact_group: str,
    dist_dir: Path,
    s3_bucket: str | None = None,
    output_dir: Path | None = None,
    dry_run: bool = False,
):
    """Uploads package files to S3 or local directory."""
    package_files = find_package_files(dist_dir)
    if not package_files:
        raise FileNotFoundError(f"No package files found in {dist_dir}")

    log(f"[INFO] Found {len(package_files)} package files in {dist_dir}:")
    for f in package_files:
        log(f"  - {f.relative_to(dist_dir)}")

    # TODO: should the `run_*_cp()` calls below use package_files instead of
    #       just copying the whole dist_dir directory?

    if s3_bucket or output_dir:
        # Explicit (override) bucket or local output, use no "external_repo" prefix.
        external_repo = ""
    else:
        external_repo, s3_bucket = retrieve_bucket_info()

    # TODO: centralize path construction within "run outputs"
    # TODO: document the structure (artifacts, logs, packages, etc.)
    path_prefix = f"{external_repo}{run_id}-{PLATFORM}/python/{artifact_group}"

    if output_dir:
        local_dist_path = output_dir / path_prefix
        run_local_cp(
            source_path=dist_dir,
            dest_path=local_dist_path,
            dry_run=dry_run,
        )
    else:
        s3_dist_path = f"s3://{s3_bucket}/{path_prefix}"
        run_aws_cp(
            source_path=dist_dir,
            s3_destination=s3_dist_path,
            dry_run=dry_run,
        )


def run(args: argparse.Namespace):
    packages_dir = args.packages_dir.resolve()
    if not packages_dir.is_dir():
        raise FileNotFoundError(f"Packages root directory not found: {packages_dir}")

    dist_dir = packages_dir / "dist"
    if not dist_dir.is_dir():
        raise FileNotFoundError(f"Packages dist/ subdirectory not found: {dist_dir}")

    log(f"[INFO] Packages directory: {packages_dir}")
    log(f"[INFO] Dist subdirectory : {dist_dir}")
    log(f"[INFO] Artifact group    : {args.artifact_group}")
    log(f"[INFO] Run ID            : {args.run_id}")
    log(f"[INFO] Platform          : {PLATFORM}")
    if args.dry_run:
        log(f"[INFO] Mode              : DRY RUN")
    elif args.output_dir:
        log(f"[INFO] Mode              : Local output to {args.output_dir}")
    else:
        log(f"[INFO] Mode              : S3 upload")

    log("")
    log("Generating pip index")
    log("--------------------")
    run_piprepo_build(dist_dir, dry_run=args.dry_run)

    log("")
    log("Uploading packages")
    log("------------------")
    upload_packages(
        run_id=args.run_id,
        artifact_group=args.artifact_group,
        dist_dir=dist_dir,
        s3_bucket=args.bucket,
        output_dir=args.output_dir,
        dry_run=args.dry_run,
    )

    # TODO: Call write_gha_upload_summary() that uses gha_append_step_summary

    log("")
    log("[INFO] Done!")


def main():
    parser = argparse.ArgumentParser(
        description="Upload Python packages to S3 or a local directory"
    )
    parser.add_argument(
        "--packages-dir",
        type=Path,
        required=True,
        help="Directory containing built packages (with dist/ subdirectory)",
    )
    parser.add_argument(
        "--artifact-group",
        type=str,
        required=True,
        help="Artifact group (e.g., gfx94X-dcgpu)",
    )
    parser.add_argument(
        "--run-id",
        type=str,
        required=True,
        help="Workflow run ID (e.g. 21440027240)",
    )
    parser.add_argument(
        "--output-dir",
        type=Path,
        default=None,
        help="Output to local directory instead of S3",
    )
    parser.add_argument(
        "--bucket",
        type=str,
        default=None,
        help="Override S3 bucket (default: auto-select via retrieve_bucket_info)",
    )
    parser.add_argument(
        "--dry-run",
        action="store_true",
        help="Print what would happen without uploading or copying",
    )

    args = parser.parse_args()

    if args.output_dir and args.bucket:
        parser.error("--output-dir and --bucket are mutually exclusive")

    run(args)


if __name__ == "__main__":
    main()
