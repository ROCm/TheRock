# Performance Drop Analysis Tool

A sophisticated performance analysis tool powered by **LangChain** and **Guardrails AI** to analyze test infrastructure performance data and identify configuration-specific and user-specific performance drops.

## Features

âœ¨ **LangChain Integration**
- Uses LangChain framework for robust LLM interactions
- Structured prompts with system and human message templates
- Token usage tracking and cost estimation
- Verbose logging for transparency

ğŸ›¡ï¸ **Guardrails AI Protection**
- Input validation to ensure data integrity
- Output validation to prevent toxic language
- Topic restriction to maintain focus on performance analysis
- Professional and actionable output enforcement

ğŸ“Š **Comprehensive Analysis**
- Configuration-specific performance drops
- User-specific performance patterns
- Hardware/OS compatibility issues
- Test failure pattern identification
- Actionable recommendations

## Installation

### 1. Clone or navigate to the repository

```bash
cd TheRock
```

### 2. Install dependencies

```bash
pip install -r requirements.txt
```

### 3. Set up your OpenAI API key

**Option A: Environment Variable (Recommended)**
```bash
# Windows PowerShell
$env:OPENAI_API_KEY="your-api-key-here"

# Windows CMD
set OPENAI_API_KEY=your-api-key-here

# Linux/Mac
export OPENAI_API_KEY="your-api-key-here"
```

**Option B: Use .env file**
Create a `.env` file in the project root:
```
OPENAI_API_KEY=your-api-key-here
```

**Option C: Pass as argument**
Use the `--api-key` flag when running the script.

## Usage

### Basic Usage

```bash
python performance_analysis.py path/to/your/data.csv
```

### With Custom Options

```bash
python performance_analysis.py path/to/your/data.csv \
    --model gpt-4o \
    --output-report my_report.md \
    --output-raw my_analysis.json
```

### Cost Control Options ğŸ’°

Control which reports are generated to manage OpenAI API costs:

```bash
# Default: Generate both reports (~$0.003 cost)
python performance_analysis.py path/to/your/data.csv

# FREE: Only test-specific report (no AI costs!)
python performance_analysis.py path/to/your/data.csv --no-ai-report

# Only AI-powered report
python performance_analysis.py path/to/your/data.csv --report-type ai-only

# Raw data only (no reports)
python performance_analysis.py path/to/your/data.csv --report-type none
```

**ğŸ“– See [COST_GUIDE.md](COST_GUIDE.md) for detailed cost comparison and recommendations.**

### Data Filtering (Zero-Test Removal)

By default, the tool performs intelligent data filtering to focus on meaningful results:

1. **Drops config columns** where NO test case executed (empty configs)
2. **Drops test rows** where the test didn't execute on ANY config (empty tests)

**Example:**
```
Before: 350 tests Ã— 120 configs (including 85 empty configs)
After:  350 tests Ã— 35 configs (only configs with test executions)
Result: More accurate failure rates!
```

To keep ALL rows and columns (including zeros):

```bash
python performance_analysis.py path/to/your/data.csv --keep-zero-rows
```

**Why filter?**
- âœ… More accurate failure rate calculations
- âœ… Focus on configs that are actually active
- âœ… Cleaner reports without empty data
- âœ… Better insights into real performance issues

### Example with the provided data

```bash
python performance_analysis.py "C:\Users\rponnuru\Downloads\SubTestCountsMatrixView.csv"
```

### Command-Line Arguments

| Argument | Description | Default |
|----------|-------------|---------|
| `csv_file` | Path to CSV file (required) | - |
| `--api-key` | OpenAI API key | Uses `OPENAI_API_KEY` env var |
| `--model` | OpenAI model to use | `gpt-4o` |
| `--output-report` | Output markdown report file | `performance_report.md` |
| `--output-raw` | Output raw JSON data file | `raw_analysis.json` |
| `--keep-zero-rows` | Keep test rows with zero tests across all configs | `False` (drops by default) |
| `--report-type` | Which reports to generate (`both`, `ai-only`, `test-only`, `none`) | `both` |
| `--no-ai-report` | Skip AI report generation to save costs (same as `--report-type test-only`) | `False` |

### Available Models

- `gpt-4o` (recommended) - Most capable, balanced cost
- `gpt-4o-mini` - Faster, lower cost
- `gpt-4-turbo` - High capability
- `gpt-3.5-turbo` - Fastest, lowest cost

## Output Files

The tool can generate up to three files (depending on options):

1. **performance_report.md** - AI-generated analysis report with (~$0.003):
   - Executive summary
   - Configuration-specific issues
   - User-specific performance analysis
   - Hardware/platform issues
   - Test-specific failures
   - Actionable recommendations
   - *(Generated only if AI report is enabled)*

2. **performance_report_test_specific.md** - Detailed test-case report (FREE):
   - Pass/fail ratios for each test
   - Specific configs where test passed vs failed
   - Pattern analysis (OS, hardware trends)
   - Performance drop observations
   - Clear tabular format
   - *(Generated by default, no AI cost)*

3. **raw_analysis.json** - Structured data (always generated):
   - Performance metrics per configuration
   - User performance statistics
   - Hardware performance data
   - Test failure details

## CSV File Format

The tool expects a CSV file with:

### Metadata Columns:
- `Features`
- `Test_Category`
- `Test_Name`
- `Software_Features`
- `Hardware_Features`
- `Test_Area`
- `Test_Execution_Mode`
- `Test_Plan_Name`
- `Execution_Label`

### Configuration Columns:
Each configuration column should follow the format:
```
Machine | OS | Hardware | GPU_Count | User | Deployment_Type
```

Example:
```
banff-123 | Ubuntu-22.04.5 | MI300X-O | 8x | John Doe | baremetal
```

## Guardrails Features

The tool implements several guardrails to ensure quality output:

### Input Validation
- Checks for required fields
- Validates data ranges
- Prevents processing of invalid data

### Output Validation
- **Toxic Language Detection** - Ensures professional language
- **Topic Restriction** - Keeps analysis focused on performance
- **Format Validation** - Ensures structured, actionable output

### Guardrails Installation (Optional but Recommended)

If guardrails are not installed, the tool will run without them but with a warning:

```bash
pip install guardrails-ai
```

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Performance Analyzer                 â”‚
â”‚                                              â”‚
â”‚  1. Data Loading (pandas)                   â”‚
â”‚  2. Performance Analysis                     â”‚
â”‚  3. Test Failure Identification              â”‚
â”‚  4. Input Validation (Guardrails)           â”‚
â”‚  5. LLM Analysis (LangChain)                â”‚
â”‚  6. Output Validation (Guardrails)          â”‚
â”‚  7. Report Generation                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Key Components

### PerformanceAnalyzer Class
Main class that orchestrates the analysis pipeline:
- Loads and parses CSV data
- Performs statistical analysis
- Integrates with LangChain for AI analysis
- Validates inputs and outputs

### PerformanceGuardrails Class
Handles all validation and safety checks:
- Input data validation
- Output quality assurance
- Topic restriction
- Toxic language prevention

### LangChain Integration
- Uses `ChatOpenAI` for LLM interactions
- Structured prompts with `ChatPromptTemplate`
- Token tracking with callbacks
- Cost estimation

## Analysis Sections

The generated report includes:

### 1. Executive Summary
Overall health and key concerns

### 2. Configuration-Specific Issues
- Configs with zero tests
- Low-performance configurations
- Common failure patterns

### 3. User-Specific Analysis
- Top and bottom performers
- Capacity issues
- Workload distribution recommendations

### 4. Hardware/Platform Issues
- Hardware-specific problems
- OS compatibility issues
- GPU configuration problems

### 5. Test-Specific Failures
- Tests failing across multiple configs
- Platform-specific failures

### 6. Actionable Recommendations
- Immediate actions
- Long-term improvements
- Resource allocation suggestions

## Troubleshooting

### "OpenAI API key not provided"
- Set the `OPENAI_API_KEY` environment variable
- Or use the `--api-key` argument

### "Guardrails AI not installed"
- The tool will still work without guardrails
- Install with: `pip install guardrails-ai`

### "CSV file not found"
- Check the file path
- Use quotes around paths with spaces
- Use absolute path if relative path doesn't work

### High token usage/cost
- Use `gpt-4o-mini` model for lower cost
- The tool displays estimated cost after analysis

## Cost Estimation

Typical analysis costs (approximate):
- **gpt-4o**: $0.10 - $0.50 per analysis
- **gpt-4o-mini**: $0.02 - $0.10 per analysis
- **gpt-4-turbo**: $0.15 - $0.70 per analysis

Actual costs depend on data size and configuration count.

## Example Output

```bash
======================================================================
Performance Drop Analysis Tool
Powered by LangChain + Guardrails
======================================================================
Loading data from SubTestCountsMatrixView.csv...
âœ“ Loaded 350 test cases across 118 configurations

Analyzing performance drops...
âœ“ Found 15 configs with zero tests
âœ“ Found 42 configs with low performance

Identifying test failures...
âœ“ Found 127 tests with high failure rates

âœ“ Raw analysis data saved to: raw_analysis.json

======================================================================
Sending data to LLM for analysis...
Model: gpt-4o
======================================================================

âœ“ Analysis complete
  - Tokens used: 3542
  - Estimated cost: $0.1245

âœ“ Output validated by guardrails
âœ“ Report saved to: performance_report.md

======================================================================
Analysis Complete!
======================================================================

Generated files:
  1. performance_report.md - AI-generated analysis report
  2. raw_analysis.json - Raw analysis data (JSON)
```

## Contributing

This tool is part of TheRock performance tracking initiative.

## License

Internal AMD use.

## Support

For issues or questions, contact the TheRock team.
