# Quick Start Guide

## ğŸš€ Get Started in 3 Steps

### Step 1: Install Dependencies

Run the setup script:

```powershell
.\setup.ps1
```

Or manually install:

```bash
pip install -r requirements.txt
```

### Step 2: Set Your API Key

```powershell
$env:OPENAI_API_KEY="your-api-key-here"
```

### Step 3: Run Analysis

```bash
python performance_analysis.py "C:\Users\rponnuru\Downloads\SubTestCountsMatrixView.csv"
```

## ğŸ“Š What You'll Get

After running the analysis, you'll get three files:

1. **performance_report.md** - AI-generated comprehensive report
2. **raw_analysis.json** - Structured data for further processing
3. **analysis_prompt.txt** - The prompt sent to the LLM (for debugging)

## ğŸ’¡ Example Output

The report includes:

âœ… **Executive Summary** - Overall infrastructure health  
âœ… **Configuration Issues** - Configs with performance drops  
âœ… **User Analysis** - Performance by user/engineer  
âœ… **Hardware Issues** - Platform-specific problems  
âœ… **Test Failures** - Tests failing across configs  
âœ… **Recommendations** - Actionable next steps  

## ğŸ¯ Key Features

### LangChain Framework
- Structured LLM interactions
- Token usage tracking
- Cost estimation
- Verbose logging

### Guardrails AI
- Input validation
- Output quality checks
- Topic restriction
- Professional output enforcement

### Comprehensive Analysis
- Config-specific drops
- User-specific patterns
- Hardware compatibility
- Test failure patterns

## ğŸ“ˆ Cost Estimates

| Model | Typical Cost |
|-------|--------------|
| gpt-4o | $0.10 - $0.50 |
| gpt-4o-mini | $0.02 - $0.10 |
| gpt-4-turbo | $0.15 - $0.70 |

## ğŸ”§ Troubleshooting

### API Key Issues
```powershell
# Check if set
echo $env:OPENAI_API_KEY

# Set it
$env:OPENAI_API_KEY="your-key"
```

### Import Errors
```bash
# Reinstall dependencies
pip install -r requirements.txt --force-reinstall
```

### CSV Format Issues
- Ensure CSV has the required metadata columns
- Check configuration column format: `Machine | OS | Hardware | GPU | User | Deployment`

## ğŸ“š More Examples

See `example_usage.py` for:
- Basic usage
- Custom API key
- Step-by-step analysis
- Data exploration (no API key needed)

## ğŸ†˜ Need Help?

1. Check **README.md** for detailed documentation
2. Review **example_usage.py** for code examples
3. Run with `--help` flag: `python performance_analysis.py --help`

## ğŸ” Security Notes

- Never commit your API key to git
- Use environment variables or .env file
- API key is only sent to OpenAI servers
- All analysis is done securely

## ğŸ“¦ What's Included

```
TheRock/
â”œâ”€â”€ performance_analysis.py    # Main analysis tool
â”œâ”€â”€ requirements.txt           # Dependencies
â”œâ”€â”€ README.md                  # Full documentation
â”œâ”€â”€ QUICKSTART.md             # This file
â”œâ”€â”€ example_usage.py          # Usage examples
â”œâ”€â”€ setup.ps1                 # Windows setup script
â””â”€â”€ .env.example             # Environment template
```

## ğŸ“ Advanced Usage

### Different Models
```bash
python performance_analysis.py data.csv --model gpt-4o-mini
```

### Custom Output Files
```bash
python performance_analysis.py data.csv \
    --output-report my_report.md \
    --output-raw my_data.json
```

### Keep All Test Rows (Including Zeros)
By default, tests with zero executions across all configs are dropped. To keep them:
```bash
python performance_analysis.py data.csv --keep-zero-rows
```

### Programmatic Usage
```python
from performance_analysis import PerformanceAnalyzer

analyzer = PerformanceAnalyzer("data.csv", model="gpt-4o")
report, stats = analyzer.run_full_analysis()
print(f"Cost: ${stats['total_cost']:.4f}")
```

## âœ¨ Next Steps

1. Run your first analysis
2. Review the generated report
3. Explore the raw JSON data
4. Customize for your needs
5. Share insights with your team

---

**Happy Analyzing! ğŸ‰**

